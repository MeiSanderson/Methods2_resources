---
title: "Class 1 - Methods 2"
author: "Pernille Brams feat. Kathrine Schulz"
date: "8/2/2024"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Users/mie/OneDrive/Skole/Methods 2 The General Linear Model/Methods2_resources/classes")

```

### Setup
```{r}
# Make sure this guy is installed/updated (if you've alreadygot rstanarm installed, you just need to load it in using either library() or p_load() as below)
#install.packages("rstanarm")
library(rstanarm)



# Load the rest
library(pacman)
pacman::p_load(tidyverse,
               ggpubr,
               ggplot2,
               stringr) # this time I'm just giving you the code
```

### Code from Chapter 1 (not exercises, just in-chapter)
```{r}
# Load data
hibbs <- read.table("data/ElectionsEconomy/data/hibbs.dat", header = TRUE)

# Make scatterplot
plot(hibbs$growth, hibbs$vote, xlab="Average recent growth in personal income",
ylab="Incumbent party's vote share")

# Estimate regression y = a + bx + error
M1 <- stan_glm(vote ~ growth, data=hibbs)

# Add a fitted line to the graph
abline(coef(M1), col="gray") # needs to be run with the plot() code above - running the whole chunk is the easiest way

# Display the fitted model
print(M1)

```

The first column shows estimates: 46.3 and 3.0 are the coefficients in the fitted line, y = 46.3 + 3.0x (see Figure 1.1b). The second column displays uncertainties in the estimates using median absolute deviations (see Section 5.3).

The last line of output shows the estimate and uncertainty of σ, the scale of the variation in the data unexplained by the regression model (the scatter of the points above and below from the regression line). This is also referred to in the book as residual standard deviation, and it is (like we know 'standard deviation' from Methods 1) a measure of the typical distance that the observed values fall from the regression line.

In Figure 1.1b, the linear model predicts vote share to roughly an accuracy of 3.9 percentage points. This means that on average, the actual values of the dependent variable (vote) deviate from the values predicted by the stan_glm model by about 3.9 percentage points.

### Code for nicer looking plot
```{r}

# Basic plot with ggplot2
ggplot(hibbs, aes(x = growth, y = vote)) +
  geom_point() +  # Add points
  labs(
    x = "Average recent growth in personal income",
    y = "Incumbent party's vote share",
    title = "Relationship between Income Growth and Vote Share",
    subtitle = "Data from Hibbs Dataset"
  ) +
  theme_minimal() +  # Use a minimal theme
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
  ) +
  geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line

```

# Exercises
These first exercises are about learning what simulating data is for.

## Ex. 0.a): In 1-2 sentences (discuss w group or reflect on your own): What does it mean to simulate data?

creating data; perhaps following a specific distribution or something.


## Ex. 0.b): Specify some amount of data points (n), some mean and some sd, and use rnorm() to simulate this amount of data points based off of the mean and sd you've set. 
```{r}
set.seed(2003) # setting a seed (in the best year ever??) - this way, even though it's random, you'll get reproducible results next time you run this with this seed

# rnorm() works like: my_simulated_data <- rnorm(n, mean, sd) - now you go!



# your code here  -  stimulating my data
n <- 2000
mean <- 7
sd <- 3

myNormalData <- rnorm(n, mean, sd)


```

## Ex. 0.c) Talk with your group and note in 1-2 sentences: What do we know of the data drawn from the rnorm()? 

we know that it follows a normal distribution and than the bulk of the data is around the mean of 7, plus that it its standard deviation.


## Ex. 0.d) Data are worth 10000% more if visualised, so let's visualise. Using ggplot, make a histogram visualising the data you simulated.

```{r}

# my histogram of my normalised data
hist(myNormalData)


```




## Ex. 0.e) Calculate the empirical mean and empirical sd (just fancy words for "calculate the mean and sd of the simulated data") - are they different from the ones you've set? and why might that be?

```{r}

# find the mean and the sd by calculating them with their respective functions

mean(myNormalData)  # gives a mean of 7.86, which differs by 0.86 from what the value I gave for the stimulation of the data

sd(myNormalData)  # gives a sd of 2.77, which differs by 0.23 from what the value I gave for the stimulation of the data


```


## Exercises from ROS
### Ex. 1.2
Sketching a regression model and data: Figure 1.1b shows data corresponding to the fitted line y = 46.3 + 3.0x with residual standard deviation 3.9, and values of x ranging roughly from 0 to 4%.
("Sketch" or "graph"= means simulate i.e. skething data means make some data yourself :) )

#### Ex. 1.2.a) Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 3.9.
* If you're not sure how to approach this exercise, try and go by the piecemeal hint-intruction in the bottom of this doc :)

```{r}

# 0) see the range of x
summary(hibbs$growth) # roughly 0 to 4%


# 1) saves the coefficients

intercept <- 30

slope <- 10


# 2) makes the x-values of the range

x <- seq(0, 4, length.out = 2000) # length.out = 20 makes 20 values, which matches the



# 3) calculates y - still without the error, and therefore it's y for the straight regression line

y_noError <- slope * x + intercept



# 4) add random error to the line above

set.seed(240)  # for reproducibility

normalisedError <- rnorm(length(x), 0, 3.9) # make normally distributed random errors w. sd of 3.9
        # length(x) is for generating the same number of values as there is for x AND 0 is the mean


y_withError <- slope * x + intercept + normalisedError # as as y_withError <- y_noError + normalisedError



# 5) plotting by putting the x- and y-values in a dataframe to prepare for plotting w. ggplot

preparedData <- data.frame(xValues = x, yModel = y_noError, yData = y_withError)
preparedData # printing of dataframe


# 6) plotting it

# Basic plot with ggplot2
ggplot(preparedData,aes(x = x)) +
  geom_point(aes(y = yData)) +# Add points
  labs(title = "my Data Plotted w. error",
       x = "Average recent groth in personal income (%)",
       y = "Incubent party's vote share") +
  theme_minimal() +  # Use a minimal theme
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
  ) +
  geom_line(aes(y = yModel), color = "orange", size = 2) # Add line of model




# 7) fit and review regression model: fit a linear regression model to the generated data by stan_glm() + see how closely the estimated parameters match the ones used to generate the data

# fitting a linear regression model to the generated data
linearRegressionModel <- stan_glm(y_withError ~ x, data=preparedData, refresh = 0)

plot(preparedData$x, preparedData$y_withError)

abline(coef(linearRegressionModel), col="purple")

print(linearRegressionModel)



# this could also be easily done with only the first line, thereby not plotting the line


    # 3.9 is seen under sigma (sd), thereby the linear regression model fit well to the generated data, seen with stan_glm()


```






#### Ex. 1.2.b) Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 10.

```{r}

### the code from above is copied - only differing in this new sd of 10, not 3.9

# -----------------------------

# 1) saves the coefficients

intercept <- 30

slope <- 10


# 2) makes the x-values of the range

x <- seq(0, 4, length.out = 2000) # length.out = 20 makes 20 values, which matches the



# 3) calculates y - still without the error, and therefore it's y for the straight regression line

y_noError <- slope * x + intercept



# 4) add random error to the line above

set.seed(240)  # for reproducibility

normalisedError <- rnorm(length(x), 0, 10) # make normally distributed random errors w. sd of 10 (NEW)
        # length(x) is for generating the same number of values as there is for x AND 0 is the mean


y_withError <- slope * x + intercept + normalisedError # as as y_withError <- y_noError + normalisedError



# 5) plotting by putting the x- and y-values in a dataframe to prepare for plotting w. ggplot

preparedData <- data.frame(xValues = x, yModel = y_noError, yData = y_withError)
preparedData # printing of dataframe


# 6) plotting it

# Basic plot with ggplot2
ggplot(preparedData,aes(x = x)) +
  geom_point(aes(y = yData)) +# Add points
  labs(title = "my Data Plotted w. error",
       x = "Average recent groth in personal income (%)",
       y = "Incubent party's vote share") +
  theme_minimal() +  # Use a minimal theme
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
  ) +
  geom_line(aes(y = yModel), color = "orange", size = 2) # Add line of model




# 7) fit and review regression model: fit a linear regression model to the generated data by stan_glm() + see how closely the estimated parameters match the ones used to generate the data

# fitting a linear regression model to the generated data
linearRegressionModel <- stan_glm(y_withError ~ x, data=preparedData, refresh = 0)

plot(preparedData$x, preparedData$y_withError)

abline(coef(linearRegressionModel), col="purple")

print(linearRegressionModel)



# this could also be easily done with only the first line, thereby not plotting the line


    # 10.1 is seen under sigma (sd), thereby the linear regression model pretty fit well to the generated data (where sd is 10.0), seen with stan_glm()



```




### Ex. 1.5 Goals of regression: Give examples of applied statistics problems of interest to you in which the goals are:
#### (a) Forecasting/classification
- predicting how many will apply to CogSci in the coming years from how many has in the previous years...


#### (b) Exploring associations
- the warmer the weather, the more murders


#### (c) Extrapolation
- prediction of the amount of future letters written


#### (d) Causal inference
- does listening to music before bed (treatment) have an effect compared to a control group that doesn't listen to music before bed



### Ex. 2.3 Data processing: Go to the data folder Names, find the file allnames_clean.csv and use this to make a graph similar to Figure 2.8, but for girls.

# task description from the solution script
Figure 2.8 shows trends of boys' names ending in each letters, showing an increase in percentage of boys names ending with the letter N. The question here asks us to do the same for girls, so we need to: 
- first make a data set with only females
- find a way to get the last letter from the column that has the name in full length, and e.g. put it in a new column
- make a plot with a line for each letter, and the same axes as in figure 2.8

```{r}

# loads in the data
allnames_clean <- read.csv("data/Names/data/allnames_clean.csv")


# step 1 - make a data frame with just the female names

head(allnames_clean)  # give the data frame a look. I find that there is a column specifying the sex

unique(allnames_clean$sex)  # checks all the different values in the sex column


 # make a new data frame with only the females
allnames_clean_FEMALE <- filter(allnames_clean, sex == 'F')

unique(allnames_clean_FEMALE$sex)  # checks that there is only females in this new data frame. There is only F



# step 2 - get the last letter in their name

 # add these last letters as a new column in the data frame
allnames_clean_FEMALE <- allnames_clean_FEMALE %>% mutate(lastLetter = str_sub(allnames_clean_FEMALE$name, -1)) # take the last letter of the names. str_sub(allnames_clean_FEMALE$name, -1)) makes a substring starting from the last character of each element in the name column


 # converts the column values to factors´for later usage
allnames_clean_FEMALE <- allnames_clean_FEMALE %>% mutate(lastLetter = as.factor(lastLetter))

class(allnames_clean_FEMALE$lastLetter)  # checks that the column is now a factor



# step 3 - make a plot with a line for each letter and the same axes as in figure 2.8 (x = years, y = percentage)

head(allnames_clean_FEMALE) # looks at how the years are represented

        # represented in a way where each year has a column showing how many has a given name in that year
head(allnames_clean_FEMALE)



 # FROM SOLUTION: Get years. The cols that are years start with X and have 4 digits after, so we can grab them using grep()
    #- which goes thru the names of female_names columns which have the pattern of "^X\\d{4}$" (that is and x and then 4 digits), while value = TRUE maes it return the years themselves and not their indices aka their placements in the dataframe

years <- grep("^X\\d{4}$", names(allnames_clean_FEMALE), value = TRUE)



 # FROM SOLUTION: Now, for each letter, let's summarise number of those per year. summarise_at() uses the summary function on multiple columns of a df
sums <- allnames_clean_FEMALE %>% group_by(lastLetter) %>% summarise_at(years, sum, na.rm = TRUE)


 # Turning the frequencies of each name per year into percentages
freq2Percent <- function(x, na.rm = TRUE) (x/sum(x))*100


 # FROM SOLUTION: here the function mutate_at() (like before) applies a function to multiple columns of a df, in this case that function is freq2Percent - this is done for sums and years
percentage_names_f<- mutate_at(sums, 
                               years, 
                               freq2Percent)

percentage_names_f



 # FROM SOLUTION: Pivot longer because it's nicer - we want one row for each year with each letter.
          # that is getting it from multiple columns to one line
long_df <- pivot_longer(percentage_names_f, cols = years, names_to = "Year", values_to = "number")

class(long_df$Year) # checks the class of year. As it has an x in it, it isn't numeric which we want.
                      # therefore we will make a a factor and then numeric

 # FROM SOLUTION:
long_df <- long_df %>% mutate(Year = as.factor(Year))
long_df <- long_df %>% mutate(Year = as.numeric(Year))

class(long_df$Year) # checks the class of year, which is now numeric
long_df$Year # checks how the data of year look - the x's have been removed and it starts over at 1 and not 1800 - FROM SOLUTION: fixed by adding 1878 to all

long_df$Year <- long_df$Year + 1879

 # now the values should match the data
min(long_df$Year) # should be 1880
max(long_df$Year) # should be 2010

long_df$Year # checks how the data of year look




# step 3.5 - the plotting

 # FROM SOLUTION:
long_df %>% 
  ggplot(aes(x = Year, y = number, col = lastLetter, group = lastLetter)) + 
  geom_line() + 
  labs(y = "Percentage of girls born with letter", 
       title = "Overview of % girls born with a specific last letter each year", 
       x = "Years") +  
  scale_x_continuous() + 
  theme(axis.text.x = element_text(angle = -45),
        plot.title = element_text(face = "bold")) # Makes title bold



```





### Ex. 2.7 (b) Reliability and validity: Give an example of a scenario of measurements that have reliability but not validity.

- reliable measurements could be a clock that is 2 hours ahead as it is precise and stable as well as no random error. But it is not a valid measure of what time it is in a specific country as it does not accurately  represent what one is trying to measure.
    However it would still be valid, if the clock was only used to measure time spent on something, which does not necessarily relate to the time of day





# Pernille's additional notes to Chapter 1 in ROS (not mandatory to read at all)
I probably wont keep making these notes this formatted but hey, here's to get you started on methods 2 transitioning from methods 1 :) They are just notes I took when skimming through the chapter.

Some of the text below mentions a lot of new words. But e.g. learning what bayesian approaches are in detail are NOT the point of methods 2. So don't worry if it doesn't make sense yet. It's not supposed to, the focus is on Bayesian in Methods 3 and 4 - and see the TL;DR on the bottom of this doc.

## The chapter
The chapter basically explains why regression is cool: it's a method used to explore a relationship between a dependent variable and one or more independent ones. The basic idea is to find the best straight line that fits some data points. Once we have the line, and if it fits well (but not too well) we can use it to make predictions. For example, if you know someone's height (X), you can plug it into the equation Y = a+bX+e to predict their weight (Y).
- Figure 1.1 in the book shows a) incumbent party's (i.e. the political party in power at time of election) vote share in some US elections, and b) a linear regression fit to these data.

## Explanations to couple the chapter with Methods 1 and the cogsci bsc in general
Note that the book uses the 'stan_glm' function to fit models. In Methods 1 we only used lm() and lmer(), and a few of you may have used glm() in your projects for e.g. logistic regressions or multinomial. 'stan_glm' is a function from the rstanarm package in R, which is used for *Bayesian generalized linear modeling.* The package allows for fitting of what we call Bayesian models using Stan, which is a programming language designed specifically for Bayesian modelling. It uses technique like Markov Chain Monte Carlo (MCMC) to sample from probability distributions. Don't think too hard about it- you'll learn more about the details of it. For now, we just use stan_glm() to fit lines to stuff.

*1. So many new terms already!! What is the difference between "Frequentist" and "Bayesian" approaches?*
Let's take lm() and stan_glm() as our two examples to explain this. The former is frequentist and proud, the latter Bayesian and proud.

**Frequentist:**
We can use lm() for linear regression. The lm() function estimates coefficients based on the data, aiming to minimise the error between the predicted and observed values (Methods 1 stuff). We express uncertainty in the lm() through confidence intervals and p-values, which are based on the concept of repeating an experiment indefinitely. The lm() cannot incorporate any prior knowledge, and as such, the analysis we can do with this one is based solely on the current data.

**Bayesian (if you're curious to know. More will come in methods 4):**
In the Bayesian framework, we can combine prior beliefs with the data we have at hand to update our knowledge about the estimates (in lm() we can only build a model on the data we have, so if the data are flawed or just straight up wrong, then we're doomed to build a really bad model).

The function stan_glm() is the lm() counterpart, just in a Bayesian framework instead of frequentist. stan_glm() incorporates so-called "prior distributions" for the estimates and uses data to update these "beliefs", resulting in so-called "posterior distributions" (which is essentially our results; it is what the model returns to us after calculating a bunch of smart stuff for us using MCMC). In stan_glm, uncertainty is NOT expressed through p-values, but through "credible intervals" (confidence intervals better cousin) derived from the posterior distributions, providing a direct probabilistic interpretation.
Prior Information: Integrates prior knowledge or beliefs through the use of prior distributions, which are then updated with the data.

**1 What is the difference between confidence intervals and credible intervals?**
- As we have seen in Methods 1, statisticians love giving names to things :)
- One could explain the difference between the two in a very long monologue (cause they *are* fundamentally statistically different), but the key point is that a) the two terms come from two different frameworks of stats, but b) have the same overall purpose: Wider intervals (be it confidence or credible) generally indicate more uncertainty about a value. Narrow means we can be more certain about a value.

*2. Why do we all of a sudden use 'stan_glm()' and not just good ol' lm()/lmer() from Methods 1?*
The function 'stan_glm' and Bayesian models in general can be more flexible in handling complex models, such as those with non-normal error distributions or hierarchical structures, that go beyond the scope of lm() and lmer(). Furthermore, in cases with small sample sizes, rare events, or multicollinearity, Bayesian methods can give us more robust and reliable estimates. You'll use stan and bayesian in general a lot more on Methods 3 and 4, so look at the use of it here as a fancy but basically the same as lm()-work, letting you get a taste of the functions. When you run stan_glm(), you will see that it samples via MCMC, which is algorithms that sample from posterior distributions of the parameters. It offers full probabilistic inference for the model parameters, including credible intervals and posterior predictive checks, which can be more informative than point estimates and confidence intervals in traditional GLMs. If all this are new words to you, good - because we haven't taught you what all that means yet fully.

*3. What is a generalised linear model (GLM) and how is it different from the lm() / lmer()'s we're used to from Methods 1?*
A Generalised Linear Model (GLM) is "just" an extension of the traditional linear model (lm() in R for example) to accommodate response variables that are not normally distributed.

You might remember that in lm(), the response is assumed to be a linear function of the predictors with normally distributed errors (remember the lovely assumption checks?). GLMs relax these constraints in two key ways:
- Different distributions: GLMs allow for response variables to follow different distributions (like binomial for binary data (data with an outcome like TRUE/FALSE, SUCCESS/FAIL, etc.), the so-called Poisson distribution for count data (data with e.g. count of friends, count of seedlings from a field of plants, etc.).
- Link function: GLMs use a so-called link function to model the relationship between the predictors and the mean of the response distribution. You'll learn more about what this is on later semesters, but the link function basically just converts (transforms) the expected value of some response variable so that it can be modeled as a linear combination of predictors, regardless of whatever scale / constraints it came from. In linear regression (lm()), we predict the response variable directly from a linear combination of predictors. However, for many types of data (like counts, binary outcomes), this direct prediction is not appropriate because the response variable has constraints (e.g., it must be positive, or between 0 and 1). So this is what we use link functions for - and this link function can be linear (like in lm()), logistic (for binary outcomes), log (for count data), etc.

Now, the lmer() model as we know from Methods 1 extends lm() by allowing for both fixed and random effects. This is useful for hierarchical or grouped data as we've seen before (see slides from Methods 1 if you need a refresher on linear mixed effects models (lmers). Thing is that GLMs can ALSO be extended to include random effects (GLMMs), so doing GLMs is basically just a more flexible way of doing the stuff we did on Methods 1. It is also cooler. And more buzzwordy. 

TL;DR: GLM/GLMMs can do everything lm() and lmer() can do, just better and you'll iteratively learn why it's better across Methods 2-3-4, trust me :) 

## Piecemeal hints
### Ex. 1.2a) stepwise-instruction
The question says: Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 3.9. We're given a linear regression equation that specifies what y should be, and we're told what the x should be from and to in the question and the info from 1.2). Here's 8 smaller steps on how to solve it, step by step: 

#### 1) First, define the regression model parameters (intercept, slope, error) (hint: just make variables called 'intercept' and so on and define the number)

#### 2) Generate X values in the specified range using seq()

#### 3) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line

#### 4) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.

#### 5) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot

#### 6) Now, plot to check it out

#### 7) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data. 
